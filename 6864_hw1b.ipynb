{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6864_hw1b",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albert-yue/6864-hw1/blob/master/6864_hw1b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N655YeL2eEUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e29b9ff-4381-4735-d39d-5a85a04a9e91"
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "rm -rf 6864-hw1\n",
        "git clone https://github.com/lingo-mit/6864-hw1.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '6864-hw1'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5R8vijdeKgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/6864-hw1\")\n",
        "\n",
        "import csv\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import lab_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJIJy6i5ygnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaoYiysseNIH",
        "colab_type": "text"
      },
      "source": [
        "## Hidden Markov Models\n",
        "\n",
        "In the remaining part of the lab (containing part 3) you'll use the Baum--Welch algorithm to learn _categorical_ representations of words in your vocabulary. Answers to questions in this lab should go in the same report as the initial release.\n",
        "\n",
        "As before, we'll start by loading up a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUn-q_pIeuAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "b011ea7d-daf2-4fdd-f30f-f345be332a0e"
      },
      "source": [
        "data = []\n",
        "n_positive = 0\n",
        "n_disp = 0\n",
        "with open(\"/content/6864-hw1/reviews.csv\") as reader:\n",
        "  csvreader = csv.reader(reader)\n",
        "  next(csvreader)\n",
        "  for id, review, label in csvreader:\n",
        "    label = int(label)\n",
        "\n",
        "    # hacky class balancing\n",
        "    if label == 1:\n",
        "      if n_positive == 2000:\n",
        "        continue\n",
        "      n_positive += 1\n",
        "    if len(data) == 4000:\n",
        "      break\n",
        "\n",
        "    data.append((review, label))\n",
        "    \n",
        "    if n_disp > 5:\n",
        "      continue\n",
        "    n_disp += 1\n",
        "    print(\"review:\", review)\n",
        "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
        "    print()\n",
        "\n",
        "print(f\"Read {len(data)} total reviews.\")\n",
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "train_reviews = reviews[:3000]\n",
        "train_labels = labels[:3000]\n",
        "val_reviews = reviews[3000:3500]\n",
        "val_labels = labels[3000:3500]\n",
        "test_reviews = reviews[3500:]\n",
        "test_labels = labels[3500:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
            "rating: 1 (good)\n",
            "\n",
            "Read 4000 total reviews.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLGB5bo7tUpj",
        "colab_type": "text"
      },
      "source": [
        "## HMM Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2qlqRHoe3y-",
        "colab_type": "text"
      },
      "source": [
        "Next, implement the forward--backward algorithm for HMMs like we saw in class.\n",
        "\n",
        "**IMPORTANT NOTE**: if you directly multiply probabilities as shown on the class slides, you'll get underflow errors. You'll probably want to work in the log domain (remember that `log(ab) = log(a) + log(b)`, `log(a+b) = logaddexp(a, b)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wVf4QVIfBdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "\n",
        "# hmm model\n",
        "class HMM(object):\n",
        "    def __init__(self, num_states, num_words):\n",
        "        self.num_states = num_states\n",
        "        self.num_words = num_words\n",
        "\n",
        "        self.states = range(num_states)\n",
        "        self.symbols = range(num_words)\n",
        "\n",
        "        self.eps = 1e-20  # small value for log probs of 0, i.e for beta_T\n",
        "\n",
        "        # initialize the matrix A with random transition probabilities p(j|i)\n",
        "        # A should be a matrix of size `num_states x num_states`\n",
        "        # with rows that sum to 1\n",
        "        self.A = np.random.rand(num_states, num_states)\n",
        "        self.A = self.A / np.sum(self.A, axis=1, keepdims=True)\n",
        "\n",
        "        # initialize the matrix B with random emission probabilities p(o|i)\n",
        "        # B should be a matrix of size `num_states x num_words`\n",
        "        # with rows that sum to 1\n",
        "        self.B = np.random.rand(num_states, num_words)\n",
        "        self.B = self.B / np.sum(self.B, axis=1, keepdims=True)\n",
        "\n",
        "        # initialize the vector pi with a random starting distribution\n",
        "        # pi should be a vector of size `num_states`\n",
        "        self.pi = np.random.rand(num_states)\n",
        "        self.pi = self.pi / np.sum(self.pi)\n",
        "\n",
        "    def generate(self, n):\n",
        "        \"\"\"randomly sample the HMM to generate a sequence.\n",
        "        \"\"\"\n",
        "        # we'll give you this one\n",
        "\n",
        "        sequence = []\n",
        "        # initialize the first state\n",
        "        state = np.random.choice(self.states, p=self.pi)\n",
        "        for i in range(n):\n",
        "            # get the emission probs for this state\n",
        "            b = self.B[state, :]\n",
        "            # emit a word\n",
        "            word = np.random.choice(self.symbols, p=b)\n",
        "            sequence.append(word)\n",
        "            # get the transition probs for this state\n",
        "            a = self.A[state, :]\n",
        "            # update the state\n",
        "            state = np.random.choice(self.states, p=a)\n",
        "        return sequence\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # run the forward algorithm\n",
        "        # this function should return a `len(obs) x num_states` matrix\n",
        "        # where the (i, j)th entry contains p(obs[:t], hidden_state_t = i)\n",
        "\n",
        "        log_alpha = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        # your code here!\n",
        "        # First time step: alpha_0(i) = pi_i B_i(o_0) for 0<=i<N\n",
        "        # in logspace: log(alpha[0, i]) = log(pi_i) + log(B[i, o_0])\n",
        "        log_alpha[0, :] = np.log(self.B[:, obs[0]]) + np.log(self.pi)\n",
        "\n",
        "        # Further time steps: alpha ...\n",
        "        for t in range(1, len(obs)):\n",
        "            # log(alpha[t-1,i]) + log(self.A[i, j])\n",
        "            # sum should match index in log_alpha with row index in self.A\n",
        "            # so the term from log_alpha should broadcast one elem to each row\n",
        "            log_trans_prob = np.log(self.A) + log_alpha[t-1, :][:, None]\n",
        "            log_trans_prob_over_prevs = logsumexp(log_trans_prob, axis=0)  # sum over 1<=i<=N\n",
        "            log_alpha[t, :] = log_trans_prob_over_prevs + np.log(self.B[:, obs[t]])\n",
        "\n",
        "        return log_alpha\n",
        "\n",
        "    def backward(self, obs):\n",
        "        # run the backward algorithm\n",
        "        # this function should return a `len(obs) x num_states` matrix\n",
        "        # where the (i, j)th entry contains p(obs[t+1:] | hidden_state_t = i)\n",
        "\n",
        "        log_beta = np.zeros((len(obs), self.num_states))\n",
        "        T = len(obs)\n",
        "\n",
        "        # beta for last time step is 1, log of which is 0\n",
        "        # log_beta[T-1, :] = np.zeros((self.num_states,))\n",
        "        \n",
        "        # Further time steps: beta[t-1,i] = sum(j=0 to N-1) A[i,j] B[j,o_t] beta[t,j]\n",
        "        for t in range(T-1, 0, -1):  # t is the time step for the future obs (=t+1 in the slides)\n",
        "            # add same value to each column (constant j value)\n",
        "            log_trans_prob = np.log(self.A) + np.log(self.B[:, obs[t]][None, :]) + log_beta[t, :][None, :]\n",
        "            log_beta[t-1, :] = logsumexp(log_trans_prob, axis=1)\n",
        "\n",
        "        return log_beta\n",
        "        \n",
        "    def forward_backward(self, obs):\n",
        "        # compute forward-backward scores\n",
        "\n",
        "        # logprob is the total log-probability of the sequence obs \n",
        "        # (marginalizing over hidden states)\n",
        "\n",
        "        # log_gamma is a matrix of size `len(obs) x num_states`\n",
        "        # it contains the marginal log probability of being in state i at time t\n",
        "\n",
        "        # log_xi is a tensor of size `len(obs) x num_states x num_states`\n",
        "        # it contains the marginal log probability of transitioning from i to j at t\n",
        "\n",
        "        log_alpha = self.forward(obs)  # T=len(obs) x num_states\n",
        "        log_beta = self.backward(obs)  # T x num_states\n",
        "        \n",
        "        logprob = logsumexp(log_alpha[len(obs)-1, :])\n",
        "\n",
        "        logprob_backward = logsumexp(np.log(self.pi) + np.log(self.B[:, obs[0]]) + log_beta[0, :])\n",
        "\n",
        "        # Compute log_xi\n",
        "        # log_xi[t,i,j] = log_alpha[t,i] + np.log(self.A[i,j]) + np.log(self.B[j,obs[t+1]]) + log_beta[t+1,j] - logprob\n",
        "        log_xi = np.zeros((len(obs), self.num_states, self.num_states))\n",
        "        \n",
        "        # shift B and beta to have info from timestep t+1 in index t\n",
        "        relevant_B = np.hstack((self.B[:, obs[1:]], np.ones((self.num_states, 1))))\n",
        "        relevant_logbeta = np.vstack((log_beta[1:, :], np.ones((1, self.num_states))))\n",
        "        \n",
        "        # change dimensions of the four matrices for broadcasting\n",
        "        new_log_A = np.tile(np.expand_dims(np.log(self.A), axis=0), (len(obs), 1, 1))\n",
        "        new_log_alpha = np.expand_dims(log_alpha, axis=2)\n",
        "        new_log_B = np.expand_dims(np.log(relevant_B.T), axis=1)\n",
        "        new_log_beta = np.expand_dims(relevant_logbeta, axis=1)\n",
        "\n",
        "        # print(new_log_A.shape)\n",
        "        # print(new_log_alpha.shape)\n",
        "        # print(new_log_B.shape)\n",
        "        # print(new_log_beta.shape)\n",
        "        log_xi = new_log_A + new_log_alpha + new_log_B + new_log_beta - logprob\n",
        "        \n",
        "        # Compute log_gamma\n",
        "        log_gamma = np.zeros((len(obs), self.num_states))\n",
        "        log_gamma = log_alpha + log_beta - logprob\n",
        "\n",
        "        return logprob, log_xi, log_gamma\n",
        "\n",
        "    def learn_unsupervised(self, corpus, num_iters, verbose=True):\n",
        "        \"\"\"Run the Baum Welch EM algorithm\n",
        "        \"\"\"\n",
        "\n",
        "        for i_iter in range(num_iters):\n",
        "            expected_si = np.full((self.num_states, ), self.eps)  # E(si -> s*): shape (num_states,)\n",
        "            expected_sij = np.full((self.num_states, self.num_states), self.eps)  # E(si -> sj): shape (num_states, num_states)\n",
        "            expected_sj = np.full((self.num_states,), self.eps)  # E(sj): shape (num_states,)\n",
        "            expected_sjwk = np.full((self.num_states, self.num_words), self.eps)  # E(sj,wk): shape (num_states, num_words)\n",
        "            total_logprob = 0\n",
        "            for i, review in enumerate(corpus):\n",
        "                logprob, log_xi, log_gamma = self.forward_backward(review)\n",
        "                # your code here \n",
        "                total_logprob += logprob\n",
        "\n",
        "                words_onehot = np.eye(self.num_words)[review]\n",
        "                max_log_gamma = np.max(log_gamma)\n",
        "                simplified_gamma = np.exp(log_gamma - max_log_gamma)\n",
        "                simp_gamma_by_word = simplified_gamma.T @ words_onehot + self.eps  # add epsilon for zeros\n",
        "                log_gamma_by_word = np.log(simp_gamma_by_word) + max_log_gamma\n",
        "                \n",
        "                if i == 0:\n",
        "                    expected_si = logsumexp(log_gamma[0:-1], axis=0)\n",
        "                    expected_sij = logsumexp(log_xi[0:-1], axis=0)\n",
        "                    expected_sj = logsumexp(log_gamma, axis=0)\n",
        "                    expected_sjwk = log_gamma_by_word\n",
        "                else:\n",
        "                    np.logaddexp(expected_si, logsumexp(log_gamma[0:-1], axis=0), out=expected_si)\n",
        "                    np.logaddexp(expected_sij, logsumexp(log_xi[0:-1], axis=0), out=expected_sij)\n",
        "                    np.logaddexp(expected_sj, logsumexp(log_gamma, axis=0), out=expected_sj)\n",
        "\n",
        "                    np.logaddexp(expected_sjwk, log_gamma_by_word, out=expected_sjwk)\n",
        "\n",
        "            if verbose: print(\"log-likelihood\", total_logprob)\n",
        "            # print(expected_sij)\n",
        "            # print(expected_si)\n",
        "            # print(expected_sjwk.shape, expected_sj.shape)\n",
        "            # print(expected_sjwk)\n",
        "            # print(expected_sj)\n",
        "            A_new = np.exp(expected_sij - expected_si[:, None])\n",
        "            B_new = np.exp(expected_sjwk - expected_sj[:, None])\n",
        "            # print(\"A_new:\", A_new)\n",
        "            # print(np.sum(A_new, axis=1, keepdims=True))\n",
        "            # print(B_new.shape)\n",
        "\n",
        "            self.A = A_new / np.sum(A_new, axis=1, keepdims=True)\n",
        "            self.B = B_new / np.sum(B_new, axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lG_Ur5YtacC",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ3hxLDyik4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "909aa7a2-03df-4fb1-885f-24e918fc46f9"
      },
      "source": [
        "corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\n",
        "hmm = HMM(num_states=2, num_words=4)\n",
        "hmm.learn_unsupervised(corpus, 1000, verbose=False)\n",
        "print(np.round(hmm.B, 2))\n",
        "print()\n",
        "hmm.generate(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.   0.5  0.27 0.23]\n",
            " [0.52 0.   0.24 0.24]]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3, 1, 1, 3, 3, 2, 1, 1, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLATL3lStt7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "239af173-be33-4334-ca4d-b2a3b2adafd8"
      },
      "source": [
        "corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\n",
        "hmm = HMM(num_states=2, num_words=4)\n",
        "\n",
        "obs = corpus[0]\n",
        "log_alpha = hmm.forward(obs)\n",
        "alpha = np.exp(log_alpha)\n",
        "print(\"alpha:\", alpha)\n",
        "print(\"pi:\", hmm.pi)\n",
        "print(\"B:\", hmm.B)\n",
        "\n",
        "# print(alpha[0, 0], hmm.pi[0] * hmm.B[0, obs[0]])\n",
        "for i in range(hmm.num_states):\n",
        "  assert np.isclose(alpha[0, i], hmm.pi[i] * hmm.B[i, obs[0]])\n",
        "\n",
        "for t in range(1, len(obs)):\n",
        "  for j in range(hmm.num_states):\n",
        "    prev_sum = 0\n",
        "    for i in range(hmm.num_states):\n",
        "      prev_sum += alpha[t-1, i] * hmm.A[i, j]\n",
        "    res = hmm.B[j, obs[t]] * prev_sum\n",
        "    # print(alpha[t,j], res)\n",
        "    assert np.isclose(alpha[t, j], res)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alpha: [[2.14839665e-01 1.38796425e-01]\n",
            " [1.47412296e-02 6.37276832e-02]\n",
            " [8.07696426e-03 2.53086511e-02]\n",
            " [1.67647572e-03 5.54125827e-03]\n",
            " [7.27979669e-04 2.35249833e-03]\n",
            " [1.55090965e-04 5.10617255e-04]\n",
            " [6.71215093e-05 2.17012015e-04]\n",
            " [1.43056829e-05 4.70967427e-05]\n",
            " [6.19100440e-06 2.00164248e-05]\n",
            " [1.31950432e-06 4.34402858e-06]\n",
            " [5.71035329e-07 1.84624115e-06]\n",
            " [1.21706207e-07 4.00677151e-07]]\n",
            "pi: [0.71762307 0.28237693]\n",
            "B: [[0.29937675 0.25750521 0.2933972  0.14972084]\n",
            " [0.49152891 0.21118213 0.0475508  0.24973817]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goZSoCDb9lzT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "fced9c58-44fc-4590-eb3f-b11a0d9aa69c"
      },
      "source": [
        "corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\n",
        "hmm = HMM(num_states=2, num_words=4)\n",
        "\n",
        "obs = corpus[0]\n",
        "log_beta = hmm.backward(obs)\n",
        "beta = np.exp(log_beta)\n",
        "print(\"beta:\", beta)\n",
        "# print(\"pi:\", hmm.pi)\n",
        "# print(\"B:\", hmm.B)\n",
        "\n",
        "for i in range(hmm.num_states):\n",
        "  assert np.isclose(beta[len(obs)-1, i], 1)\n",
        "\n",
        "for t in range(len(obs)-2, -1, -1):\n",
        "  for i in range(hmm.num_states):\n",
        "    fut_sum = 0\n",
        "    for j in range(hmm.num_states):\n",
        "      fut_sum += hmm.A[i,j] * hmm.B[j, obs[t+1]] * beta[t+1, j]\n",
        "    assert np.isclose(beta[t, i], fut_sum)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beta: [[2.70381525e-05 2.98880927e-05]\n",
            " [9.17308160e-05 9.48983330e-05]\n",
            " [1.72712937e-04 1.90917640e-04]\n",
            " [5.85953447e-04 6.06186749e-04]\n",
            " [1.10324694e-03 1.21953399e-03]\n",
            " [3.74292421e-03 3.87216870e-03]\n",
            " [7.04727104e-03 7.79007335e-03]\n",
            " [2.39090566e-02 2.47343684e-02]\n",
            " [4.50184099e-02 4.97596750e-02]\n",
            " [1.52793548e-01 1.57971024e-01]\n",
            " [2.88340511e-01 3.17376018e-01]\n",
            " [1.00000000e+00 1.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grza4fuq-416",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "81a3f0a4-c2c7-413f-c429-297274785877"
      },
      "source": [
        "from scipy.special import logsumexp\n",
        "\n",
        "corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\n",
        "hmm = HMM(num_states=2, num_words=4)\n",
        "\n",
        "obs = corpus[0]\n",
        "log_alpha = hmm.forward(obs)\n",
        "alpha = np.exp(log_alpha)\n",
        "log_beta = hmm.backward(obs)\n",
        "beta = np.exp(log_beta)\n",
        "\n",
        "prob_forward = 0\n",
        "for i in range(hmm.num_states):\n",
        "  prob_forward += alpha[len(obs)-1, i]\n",
        "\n",
        "prob_backward = 0\n",
        "for i in range(hmm.num_states):\n",
        "  prob_backward += hmm.pi[i] * hmm.B[i, obs[0]] * beta[0, i]\n",
        "\n",
        "print(prob_forward, prob_backward)\n",
        "assert np.isclose(prob_forward, prob_backward)\n",
        "print(np.log(prob_forward), np.log(prob_backward))\n",
        "\n",
        "logprob_forward = logsumexp(log_alpha[len(obs)-1, :])\n",
        "logprob_backward = logsumexp(np.log(hmm.pi) + np.log(hmm.B[:, obs[0]]) + log_beta[0, :])\n",
        "\n",
        "print(logprob_forward)\n",
        "print(logprob_backward)\n",
        "\n",
        "assert np.isclose(np.log(prob_forward), logprob_forward)\n",
        "assert np.isclose(np.log(prob_forward), logprob_backward)\n",
        "assert np.isclose(np.log(prob_backward), logprob_forward)\n",
        "assert np.isclose(np.log(prob_backward), logprob_backward)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.429158639789488e-07 2.42915863978949e-07\n",
            "-15.230550692330104 -15.230550692330102\n",
            "-15.230550692330104\n",
            "-15.230550692330102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE2H2mqgE5dC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a0b494c8-97bb-4ebb-e244-e5c164f4a473"
      },
      "source": [
        "corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\n",
        "hmm = HMM(num_states=2, num_words=4)\n",
        "\n",
        "obs = corpus[0]\n",
        "\n",
        "log_alpha = hmm.forward(obs)  # T=len(obs) x num_states\n",
        "log_beta = hmm.backward(obs)  # T x num_states\n",
        "\n",
        "logprob = logsumexp(log_alpha[len(obs)-1, :])\n",
        "log_gamma = np.zeros((len(obs), hmm.num_states))\n",
        "log_xi = np.zeros((len(obs), hmm.num_states, hmm.num_states))\n",
        "\n",
        "logprob_backward = logsumexp(np.log(hmm.pi) + np.log(hmm.B[:, obs[0]]) + log_beta[0, :])\n",
        "\n",
        "# log_xi[t,i,j] = log_alpha[t,i] + np.log(self.A[i,j]) + np.log(self.B[j,obs[t+1]]) + log_beta[t+1,j] - logprob\n",
        "relevant_B = np.hstack((hmm.B[:, obs[1:]], np.ones((hmm.num_states, 1))))\n",
        "# print(relevant_B)\n",
        "# print(np.log(relevant_B))\n",
        "relevant_logbeta = np.vstack((log_beta[1:, :], np.zeros((1, hmm.num_states))))\n",
        "# print(log_beta)\n",
        "# print(relevant_logbeta)\n",
        "\n",
        "# print(np.tile(np.expand_dims(np.log(hmm.A), axis=0), (len(obs), 1, 1)).shape)\n",
        "# print(np.tile(np.expand_dims(np.log(hmm.A), axis=0), len(obs)))\n",
        "new_log_A = np.tile(np.expand_dims(np.log(hmm.A), axis=0), (len(obs), 1, 1))\n",
        "\n",
        "new_log_alpha = np.expand_dims(log_alpha, axis=2)\n",
        "# print(new_log_alpha.shape)\n",
        "# print(new_log_alpha[0])\n",
        "\n",
        "# print(new_log_A[0])\n",
        "# print((new_log_A + new_log_alpha)[0])\n",
        "\n",
        "new_log_B = np.expand_dims(np.log(relevant_B.T), axis=1)\n",
        "# print(new_log_B)\n",
        "\n",
        "new_log_beta = np.expand_dims(relevant_logbeta, axis=1)\n",
        "# print(new_log_beta.shape)\n",
        "\n",
        "log_xi = new_log_A + new_log_alpha + new_log_B + new_log_beta - logprob\n",
        "\n",
        "# log_xi[t,:,:] = np.log(hmm.A) + log_alpha[t, :][:, None] + np.log(relevant_B)[:, t][None, :] + relevant_logbeta[t, :][None, :] - logprob\n",
        "\n",
        "t=0\n",
        "log_expected = np.zeros((hmm.num_states, hmm.num_states))\n",
        "for i in range(hmm.num_states):\n",
        "  for j in range(hmm.num_states):\n",
        "    log_expected[i,j] = np.log(hmm.A[i,j]) + log_alpha[t, i] + np.log(hmm.B[j, obs[t+1]]) + log_beta[t+1, j] - logprob\n",
        "\n",
        "\n",
        "print(log_xi[0,:,:])\n",
        "print(log_expected)\n",
        "# print(\"-----\")\n",
        "# print(np.log(hmm.A))\n",
        "# print(log_alpha[t, :][:, None])\n",
        "# print(np.log(relevant_B)[:, t][None, :])\n",
        "# print(relevant_logbeta[t, :][None, :])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.20548595 -2.03378722]\n",
            " [-1.57088655 -1.01679165]]\n",
            "[[-1.20548595 -2.03378722]\n",
            " [-1.57088655 -1.01679165]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AX8NwCmB_PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\n",
        "hmm = HMM(num_states=2, num_words=4)\n",
        "\n",
        "obs = corpus[0]\n",
        "\n",
        "log_alpha = hmm.forward(obs)\n",
        "alpha = np.exp(log_alpha)\n",
        "logprob_forward = logsumexp(log_alpha[len(obs)-1, :])\n",
        "\n",
        "log_beta = hmm.backward(obs)\n",
        "beta = np.exp(log_beta)\n",
        "\n",
        "logprob, log_xi, log_gamma = hmm.forward_backward(obs)\n",
        "xi = np.exp(log_xi)\n",
        "gamma = np.exp(log_gamma)\n",
        "\n",
        "# make sure logprob matches what we'd expect \n",
        "assert np.isclose(logprob, logprob_forward)\n",
        "\n",
        "# values for t = T (i.e. len(obs)-1) don't matter for updates\n",
        "# full_log_expected = np.zeros((len(obs), hmm.num_states, hmm.num_states))\n",
        "for t in range(len(obs)-1):\n",
        "  for i in range(hmm.num_states):\n",
        "    for j in range(hmm.num_states):\n",
        "      expected = alpha[t, i] * hmm.A[i,j] * hmm.B[j, obs[t+1]] * beta[t+1, j] / np.exp(logprob)\n",
        "      log_expected = log_alpha[t, i] + np.log(hmm.A[i,j]) + np.log(hmm.B[j, obs[t+1]]) + log_beta[t+1, j] - logprob\n",
        "      # full_log_expected[t,i,j] = log_expected\n",
        "      assert np.isclose(log_xi[t, i, j], log_expected), \"Expected log_xi value: {} but got {}\".format(log_expected, log_xi[t, i, j])\n",
        "      assert np.isclose(xi[t, i, j], expected), \"Expected xi value: {} but got {}\".format(expected, xi[t, i, j])\n",
        "\n",
        "# for t in range(len(obs)):\n",
        "#   print('t =', t)\n",
        "#   print(log_xi[t])\n",
        "#   print(full_log_expected[t])\n",
        "#   print('---')\n",
        "\n",
        "# skip the last time step, since xi won't match here\n",
        "for t in range(len(obs)-1):\n",
        "  for i in range(hmm.num_states):\n",
        "    expected = 0\n",
        "    for j in range(hmm.num_states):\n",
        "      expected += xi[t,i,j]\n",
        "    assert np.isclose(gamma[t,i], expected), \"At time {} and state {}, expected {} but got {}\".format(t, i, expected, gamma[t,i])\n",
        "\n",
        "assert np.allclose(np.sum(gamma, axis=1), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9WEoJSIVkoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f663642d-fef5-4981-956d-d70624daf3f4"
      },
      "source": [
        "# Not sure what to set the last column in relevant_B and relevant_logbeta\n",
        "# to get the right values for the last time step\n",
        "gamma_from_xi_last_time = np.zeros((hmm.num_states,))\n",
        "for i in range(hmm.num_states):\n",
        "  expected = 0\n",
        "  for j in range(hmm.num_states):\n",
        "    expected += xi[-1,i,j]\n",
        "  gamma_from_xi_last_time[i] = expected\n",
        "\n",
        "print(gamma_from_xi_last_time)\n",
        "print(gamma[-1, :])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.28970038 1.42858145]\n",
            "[0.47445426 0.52554574]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JhRgXrFq3Zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\n",
        "hmm = HMM(num_states=2, num_words=4)\n",
        "\n",
        "total_logprob = 0\n",
        "for i, review in enumerate(corpus):\n",
        "    logprob, log_xi, log_gamma = hmm.forward_backward(review)\n",
        "    # your code here \n",
        "    total_logprob += logprob\n",
        "\n",
        "    words_onehot = np.eye(hmm.num_words)[review]\n",
        "    max_log_gamma = np.max(log_gamma)\n",
        "    simplified_gamma = np.exp(log_gamma - max_log_gamma)\n",
        "    simp_gamma_by_word = simplified_gamma.T @ words_onehot + hmm.eps  # add epsilon for zeros\n",
        "    log_gamma_by_word = np.log(simp_gamma_by_word) + max_log_gamma\n",
        "    \n",
        "    if i == 0:\n",
        "        expected_si = logsumexp(log_gamma[0:-1], axis=0)\n",
        "        expected_sij = logsumexp(log_xi[0:-1], axis=0)\n",
        "        expected_sj = logsumexp(log_gamma, axis=0)\n",
        "        expected_sjwk = log_gamma_by_word\n",
        "    else:\n",
        "        np.logaddexp(expected_si, logsumexp(log_gamma[0:-1], axis=0), out=expected_si)\n",
        "        np.logaddexp(expected_sij, logsumexp(log_xi[0:-1], axis=0), out=expected_sij)\n",
        "        np.logaddexp(expected_sj, logsumexp(log_gamma, axis=0), out=expected_sj)\n",
        "\n",
        "        np.logaddexp(expected_sjwk, log_gamma_by_word, out=expected_sjwk)\n",
        "\n",
        "wanted_si = np.zeros((hmm.num_states, ))\n",
        "wanted_sij = np.zeros((hmm.num_states, hmm.num_states))  # E(si -> sj): shape (num_states, num_states)\n",
        "wanted_sj = np.zeros((hmm.num_states,))  # E(sj): shape (num_states,)\n",
        "wanted_sjwk = np.zeros((hmm.num_states, hmm.num_words))  # E(sj,wk): shape (num_states, num_words)\n",
        "for idx, review in enumerate(corpus):\n",
        "    log_alpha = hmm.forward(review)\n",
        "    alpha = np.exp(log_alpha)\n",
        "    logprob_forward = logsumexp(log_alpha[len(review)-1, :])\n",
        "\n",
        "    log_beta = hmm.backward(review)\n",
        "    beta = np.exp(log_beta)\n",
        "\n",
        "    logprob, log_xi, log_gamma = hmm.forward_backward(review)\n",
        "    xi = np.exp(log_xi)\n",
        "    gamma = np.exp(log_gamma)\n",
        "\n",
        "    for i in range(hmm.num_states):\n",
        "        for t in range(len(review)-1):\n",
        "            wanted_si[i] += gamma[t,i]\n",
        "    \n",
        "    for i in range(hmm.num_states):\n",
        "        for j in range(hmm.num_states):\n",
        "            for t in range(len(review)-1):\n",
        "                wanted_sij[i,j] += xi[t,i,j]\n",
        "    \n",
        "    for j in range(hmm.num_states):\n",
        "        for t in range(len(review)):\n",
        "            wanted_sj[j] += gamma[t,j]\n",
        "\n",
        "    # if idx != 0: continue\n",
        "    # print(gamma)\n",
        "    # print(review)\n",
        "    for j in range(hmm.num_states):\n",
        "        for w in range(hmm.num_words):\n",
        "            for t in range(len(review)):\n",
        "                if review[t] == w:\n",
        "                    wanted_sjwk[j, w] += gamma[t,j]\n",
        "\n",
        "for i in range(hmm.num_states):\n",
        "    # print(expected_si[i], np.log(wanted_si[i]))\n",
        "    # print(np.exp(expected_si[i]), wanted_si[i])\n",
        "    assert np.isclose(expected_si[i], np.log(wanted_si[i]))\n",
        "\n",
        "for i in range(hmm.num_states):\n",
        "    for j in range(hmm.num_states):\n",
        "        assert np.isclose(expected_sij[i,j], np.log(wanted_sij[i,j]))\n",
        "\n",
        "# print(expected_sj)\n",
        "# print(np.log(wanted_sj))\n",
        "for j in range(hmm.num_states):\n",
        "    assert np.isclose(expected_sj[j], np.log(wanted_sj[j]))\n",
        "\n",
        "# print(expected_sjwk)\n",
        "# print(np.log(wanted_sjwk))\n",
        "for i in range(hmm.num_states):\n",
        "    for w in range(hmm.num_words):\n",
        "        assert np.isclose(expected_sjwk[i,w], np.log(wanted_sjwk[i,w]))\n",
        "\n",
        "# np.logaddexp(expected_sij, logsumexp(log_xi[0:-1], axis=0), out=expected_sij)\n",
        "# np.logaddexp(expected_sj, logsumexp(log_gamma, axis=0), out=expected_sj)\n",
        "\n",
        "# words_onehot = np.eye(self.num_words)[review]\n",
        "# max_log_gamma = np.max(log_gamma)\n",
        "# simplified_gamma = np.exp(log_gamma - max_log_gamma)\n",
        "# simp_gamma_by_word = simplified_gamma.T @ words_onehot + self.eps  # add epsilon for zeros\n",
        "# log_gamma_by_word = np.log(simp_gamma_by_word) + max_log_gamma\n",
        "\n",
        "# np.logaddexp(expected_sjwk, log_gamma_by_word, out=expected_sjwk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAtrPXG2tfm4",
        "colab_type": "text"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-l7WucpCBP",
        "colab_type": "text"
      },
      "source": [
        "Train a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTWXUt15pDg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "1990095d-4aed-4d75-8063-06e6bd502296"
      },
      "source": [
        "tokenizer = lab_util.Tokenizer()\n",
        "tokenizer.fit(train_reviews)\n",
        "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
        "print(tokenizer.vocab_size)\n",
        "\n",
        "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2006\n",
            "log-likelihood -2089917.1991944504\n",
            "log-likelihood -1524887.1304268788\n",
            "log-likelihood -1524185.523647732\n",
            "log-likelihood -1523412.772067245\n",
            "log-likelihood -1522495.8381112337\n",
            "log-likelihood -1521350.8320171814\n",
            "log-likelihood -1519871.4118507395\n",
            "log-likelihood -1517913.31245186\n",
            "log-likelihood -1515272.647603099\n",
            "log-likelihood -1511661.0869601287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiCwE05xqXmI",
        "colab_type": "text"
      },
      "source": [
        "Let's look at some of the words associated with each hidden state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhMoLUFqbn_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27049d26-45ce-4e54-9e57-a0ac3ee8fa2d"
      },
      "source": [
        "for i in range(hmm.num_states):\n",
        "    most_probable = np.argsort(hmm.B[i, :])[::-1][:10]\n",
        "    print(f\"state {i}\")   \n",
        "    for o in most_probable:\n",
        "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
        "    print()"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state 0\n",
            ". 0.09441054242543682\n",
            "i 0.07375932838554647\n",
            "<unk> 0.035941882391585966\n",
            "it 0.03331877851197925\n",
            ", 0.03106381307967656\n",
            "they 0.03023826336974981\n",
            "that 0.022137165229001256\n",
            "and 0.020421180373083766\n",
            "not 0.019489947629475077\n",
            "you 0.014195313828903622\n",
            "\n",
            "state 1\n",
            "br 0.12176408273555041\n",
            ", 0.09291446561364468\n",
            "<unk> 0.09034711996799845\n",
            ". 0.0659456278307458\n",
            "the 0.033882367610778695\n",
            "but 0.024677347973385864\n",
            "i 0.021000502852190524\n",
            "this 0.01950368593574959\n",
            "is 0.019205750882931572\n",
            "it 0.018451345223994327\n",
            "\n",
            "state 2\n",
            ". 0.08378024253532976\n",
            "the 0.08316872013124443\n",
            "<unk> 0.06267266463746844\n",
            "i 0.04092554562746479\n",
            "to 0.02792907382439079\n",
            "of 0.02121551827847617\n",
            "and 0.020749482167022375\n",
            "this 0.017798106510406583\n",
            ", 0.01753363453933246\n",
            "my 0.015819512005304567\n",
            "\n",
            "state 3\n",
            "a 0.06877201431996974\n",
            "<unk> 0.054663486178699\n",
            "and 0.053593120003017795\n",
            "the 0.05108793288252669\n",
            "to 0.04970410656083046\n",
            ". 0.038678973627255274\n",
            ", 0.03763810530416528\n",
            "not 0.034643715563881336\n",
            "in 0.02224082604480798\n",
            "little 0.013879539547936607\n",
            "\n",
            "state 4\n",
            "<unk> 0.13149855982052133\n",
            ". 0.03951301015366121\n",
            "a 0.029893492237588772\n",
            "the 0.02807700850888984\n",
            "like 0.018056999785343253\n",
            "in 0.017749000235563108\n",
            "and 0.01517227919892736\n",
            "but 0.01344810535555898\n",
            "was 0.012246967951761917\n",
            "for 0.011682937377019264\n",
            "\n",
            "state 5\n",
            ". 0.10667177103825425\n",
            "<unk> 0.06465331901051229\n",
            "i 0.05643537276107894\n",
            "the 0.04081942118781156\n",
            "is 0.034156494440733376\n",
            "a 0.023385125891950808\n",
            "and 0.022010645483575623\n",
            "are 0.020154187103142034\n",
            "was 0.02015102416965205\n",
            "this 0.018204334497458795\n",
            "\n",
            "state 6\n",
            "<unk> 0.11426570228511333\n",
            "the 0.051663134016225847\n",
            "of 0.033235904580227016\n",
            "it 0.028984588403912142\n",
            "to 0.027829320419171282\n",
            "this 0.02267960462618376\n",
            "my 0.022616415436390824\n",
            "and 0.018686126427179088\n",
            "for 0.014581281233718126\n",
            "them 0.012525630367102922\n",
            "\n",
            "state 7\n",
            "<unk> 0.1287932321469229\n",
            ". 0.07381385411044272\n",
            "the 0.06182351637889356\n",
            "and 0.0489805037917137\n",
            ", 0.0405169611089532\n",
            "in 0.02483564751060873\n",
            "it 0.019576604387935674\n",
            "that 0.012979725214314786\n",
            "i 0.012120338900106125\n",
            "just 0.011333972990936265\n",
            "\n",
            "state 8\n",
            "i 0.08384585719339639\n",
            "a 0.056264733361288295\n",
            ", 0.05554737280765681\n",
            "have 0.030017850935334787\n",
            "and 0.025733676322805956\n",
            "the 0.021309160630984023\n",
            "is 0.019070421754305494\n",
            "you 0.01872634839399412\n",
            ". 0.017973985634848526\n",
            "! 0.013661761205371324\n",
            "\n",
            "state 9\n",
            ". 0.09897079245710175\n",
            "<unk> 0.07636008119633064\n",
            "of 0.07614743306791881\n",
            ", 0.06818750043155516\n",
            "to 0.04269985606081056\n",
            "it 0.03294419709213846\n",
            "a 0.030849564461663002\n",
            "for 0.025972426870009266\n",
            "on 0.020302482647297247\n",
            "and 0.01767813874185242\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAQ_PmASwdFz",
        "colab_type": "text"
      },
      "source": [
        "We can also look at some samples from the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj1eT3s3wgFJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "0092b41a-bf57-4199-aaee-718af3788459"
      },
      "source": [
        "for i in range(10):\n",
        "    print(tokenizer.de_tokenize([hmm.generate(10)]))"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['during <unk> i up all , br i in single']\n",
            "['just <unk> baby bag for salty ! tastes want go']\n",
            "['cats plus away i get <unk> show found i stick']\n",
            "['form <unk> <unk> when yes i times would little and']\n",
            "['snack salt changed saw not , ! from have spent']\n",
            "['<unk> the choices and a <unk> company or easily do']\n",
            "['spread good healthier being but pack com my had just']\n",
            "['<unk> less , . around ! <unk> much taste other']\n",
            "['i bag them immediately protein my you taste . sodas']\n",
            "['along if but pancake would the , some healthier unpleasant']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Qk9adNr7lQ",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's repeat the classification experiment from Parts 1 and 2, using the _vector of expected hidden state counts_ as a sentence representation.\n",
        "\n",
        "(Warning! results may not be the same as in earlier versions of this experiment.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL6JQXLJspyA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "1565533b-f63f-4cac-c9b9-295d3772f8ae"
      },
      "source": [
        "def train_model(xs_featurized, ys):\n",
        "  import sklearn.linear_model\n",
        "  model = sklearn.linear_model.LogisticRegression()\n",
        "  model.fit(xs_featurized, ys)\n",
        "  return model\n",
        "\n",
        "def eval_model(model, xs_featurized, ys, verbose=True):\n",
        "  pred_ys = model.predict(xs_featurized)\n",
        "  if verbose: print(\"test accuracy\", np.mean(pred_ys == ys))\n",
        "  return np.mean(pred_ys == ys)\n",
        "\n",
        "def training_experiment(name, featurizer, n_train, verbose=True):\n",
        "    if verbose: print(f\"{name} features, {n_train} examples\")\n",
        "    train_xs = np.array([\n",
        "        hmm_featurizer(tokenizer.tokenize([review])) \n",
        "        for review in train_reviews[:n_train]\n",
        "    ])\n",
        "    train_ys = train_labels[:n_train]\n",
        "    test_xs = np.array([\n",
        "        hmm_featurizer(tokenizer.tokenize([review]))\n",
        "        for review in test_reviews\n",
        "    ])\n",
        "    test_ys = test_labels\n",
        "    model = train_model(train_xs, train_ys)\n",
        "    acc = eval_model(model, test_xs, test_ys)\n",
        "    if verbose: print()\n",
        "    return acc\n",
        "\n",
        "def hmm_featurizer(review):\n",
        "    review = review[0]\n",
        "    _, _, gamma = hmm.forward_backward(review)\n",
        "    return gamma.sum(axis=0)\n",
        "\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=100)"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hmm features, 100 examples\n",
            "test accuracy 0.508\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.508"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W0qx41Bu2aF",
        "colab_type": "text"
      },
      "source": [
        "**Part 3: Lab writeup**\n",
        "\n",
        "1. What do the learned hidden states seem to encode when you run unsupervised \n",
        "   HMM training with only 2 states? What about 10? What about 100?\n",
        "\n",
        "2. As before, what's the relationship between # of labeled examples and    \n",
        "   usefulness of HMM-based sentence representations? Are these results generally\n",
        "   better or worse than in Parts 1 and 2 of the homework? Why or why not might\n",
        "   HMM state distributions be sensible sentence representations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoGyY9ofzFEy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c85d262d-330e-49a4-fb05-a581f6de0588"
      },
      "source": [
        "# To find lower limit for logistic regression convergence\n",
        "train_sizes = [200, 300, 500, 750, 1000, 1500, 2000, 2500, 3000]\n",
        "\n",
        "for n in train_sizes:\n",
        "    training_experiment(\"hmm\", hmm_featurizer, n_train=n)"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hmm features, 200 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.574\n",
            "\n",
            "hmm features, 300 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.642\n",
            "\n",
            "hmm features, 500 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.602\n",
            "\n",
            "hmm features, 750 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.606\n",
            "\n",
            "hmm features, 1000 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.612\n",
            "\n",
            "hmm features, 1500 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.614\n",
            "\n",
            "hmm features, 2000 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.618\n",
            "\n",
            "hmm features, 2500 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.616\n",
            "\n",
            "hmm features, 3000 examples\n",
            "test accuracy 0.616\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTzK27AKyXhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ca9ec88-d163-46a8-c307-1bd78642851a"
      },
      "source": [
        "# jk looks like all are bad for #states=10\n",
        "train_sizes = [10, 20, 30, 50, 75, 100, 200, 300, 500, 750, 1000, 1500, 2000, 2500, 3000]\n",
        "\n",
        "hmm_10_results = []\n",
        "\n",
        "for n in train_sizes:\n",
        "    hmm_10_results.append(training_experiment(\"hmm\", hmm_featurizer, n_train=n, verbose=False))"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.616\n",
            "test accuracy 0.616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCMZLUPlyYNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "a5496776-bdda-402b-acee-e0b11f7c91d3"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(train_sizes, hmm_10_results, label=\"hmm\")\n",
        "# plt.legend(loc = \"lower right\")\n",
        "# plt.title('Test Accuracy of HMM-based Representations with 10 Hidden States on Different Train Data Sizes')\n",
        "plt.xlabel('Number of Train Examples')\n",
        "plt.ylabel('Test Accuracy')\n",
        "\n",
        "# plt.savefig('hmm_ntrain.png')\n",
        "# files.download('hmm_ntrain.png')\n",
        "plt.show()"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAFzCAYAAAAkFp78AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAAhIiIlKUCIiEhJ/x9r+6ezX+PIkQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d50X7EvpNxKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "1510d0fe-33c3-475c-eea0-89dde2c44125"
      },
      "source": [
        "tokenizer = lab_util.Tokenizer()\n",
        "tokenizer.fit(train_reviews)\n",
        "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
        "print(tokenizer.vocab_size)\n",
        "\n",
        "hmm2 = HMM(num_states=2, num_words=tokenizer.vocab_size)\n",
        "hmm2.learn_unsupervised(train_reviews_tk, 10)\n",
        "\n",
        "for i in range(hmm2.num_states):\n",
        "    most_probable_2 = np.argsort(hmm2.B[i, :])[::-1][:10]\n",
        "    print(f\"state {i}\")   \n",
        "    for o in most_probable_2:\n",
        "        print(tokenizer.token_to_word[o], hmm2.B[i, o])\n",
        "    print()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2006\n",
            "log-likelihood -2091938.7828791158\n",
            "log-likelihood -1525759.6139374922\n",
            "log-likelihood -1525452.544514095\n",
            "log-likelihood -1525196.4582913124\n",
            "log-likelihood -1524963.8515883384\n",
            "log-likelihood -1524736.7491449362\n",
            "log-likelihood -1524502.0284070147\n",
            "log-likelihood -1524249.7522469894\n",
            "log-likelihood -1523972.7658050563\n",
            "log-likelihood -1523666.0009118551\n",
            "state 0\n",
            "<unk> 0.10173966058336177\n",
            "and 0.043608580042030824\n",
            "the 0.029500988164168577\n",
            "i 0.028734435183907198\n",
            "it 0.028385692301300905\n",
            "to 0.02548693917115605\n",
            "of 0.02425388747515287\n",
            ". 0.019874445558347602\n",
            "br 0.0189519945042716\n",
            "for 0.013088892881360028\n",
            "\n",
            "state 1\n",
            ". 0.11489358919612605\n",
            ", 0.06314141567349009\n",
            "<unk> 0.050067477457172715\n",
            "the 0.04707051540179577\n",
            "a 0.034755652784691886\n",
            "i 0.030292078886628146\n",
            "is 0.021600135446078216\n",
            "this 0.017817989953622652\n",
            "that 0.013896192304025948\n",
            "as 0.010761163716723036\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTE1qOMTI4BU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "5d286e79-2ff3-4c82-a7ce-468568b86827"
      },
      "source": [
        "for i in range(10):\n",
        "    print(tokenizer.de_tokenize([hmm.generate(10)]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['not <unk> <unk> are are <unk> are not are <unk>']\n",
            "['are are <unk> these <unk> not not are are are']\n",
            "['not are not <unk> <unk> <unk> not are not <unk>']\n",
            "['are not <unk> are are not not are <unk> these']\n",
            "['<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> not not']\n",
            "['<unk> not not are <unk> <unk> not are not not']\n",
            "['<unk> not are <unk> not not not <unk> <unk> not']\n",
            "['<unk> <unk> <unk> <unk> not <unk> these <unk> <unk> are']\n",
            "['are not <unk> not not not <unk> are <unk> not']\n",
            "['not not these <unk> <unk> <unk> these <unk> not <unk>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0e9Xx5uN724",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "42a3fe3c-30b0-4a22-cd2a-26af2717d0fe"
      },
      "source": [
        "tokenizer50 = lab_util.Tokenizer()\n",
        "tokenizer50.fit(train_reviews)\n",
        "train_reviews_tk = tokenizer50.tokenize(train_reviews)\n",
        "print(tokenizer50.vocab_size)\n",
        "\n",
        "hmm50 = HMM(num_states=50, num_words=tokenizer50.vocab_size)\n",
        "hmm50.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2006\n",
            "log-likelihood -2078086.4314356584\n",
            "log-likelihood -1525345.931128712\n",
            "log-likelihood -1525212.8916231303\n",
            "log-likelihood -1525077.4076970222\n",
            "log-likelihood -1524935.6948303515\n",
            "log-likelihood -1524783.3620497612\n",
            "log-likelihood -1524614.9498223907\n",
            "log-likelihood -1524423.254373716\n",
            "log-likelihood -1524198.2785475415\n",
            "log-likelihood -1523925.5468200485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXSiPuZ2ubMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed538ea3-7d6f-4232-c819-c280d56501bb"
      },
      "source": [
        "for i in range(hmm50.num_states):\n",
        "    most_probable = np.argsort(hmm50.B[i, :])[::-1][:10]\n",
        "    print(f\"state {i}\")   \n",
        "    for o in most_probable:\n",
        "        print(tokenizer50.token_to_word[o], hmm50.B[i, o])\n",
        "    print()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state 0\n",
            ". 0.13357598539401402\n",
            "and 0.04970710984142213\n",
            "to 0.03713581602758531\n",
            "i 0.034144943706297386\n",
            "it 0.03213097577305458\n",
            "<unk> 0.020755137834818856\n",
            "a 0.015988097084972275\n",
            ", 0.015592170407548204\n",
            "the 0.015038233535990209\n",
            "in 0.012332338550948951\n",
            "\n",
            "state 1\n",
            "the 0.07098796264524891\n",
            "<unk> 0.05716581508292614\n",
            ". 0.048512197025335664\n",
            "a 0.04809956021703975\n",
            "and 0.03629223329029702\n",
            "it 0.0353653557046032\n",
            ", 0.02844681053321185\n",
            "is 0.01922100994796981\n",
            "in 0.016030475683709956\n",
            "to 0.015323226765717315\n",
            "\n",
            "state 2\n",
            "the 0.06906881457874227\n",
            ", 0.06146344152279113\n",
            "<unk> 0.04452558688897632\n",
            "it 0.03938063072879583\n",
            ". 0.034586832626620316\n",
            "and 0.026818188668161786\n",
            "for 0.021916794594798044\n",
            "in 0.02032497462957678\n",
            "to 0.018473802180453323\n",
            "with 0.013388953633070222\n",
            "\n",
            "state 3\n",
            "i 0.09197383181700625\n",
            "<unk> 0.08941947287110451\n",
            ", 0.05365318935811315\n",
            ". 0.038321170103563514\n",
            "of 0.03149647371717822\n",
            "a 0.0173919461158527\n",
            "these 0.016526935189500446\n",
            "this 0.016473758166895586\n",
            "and 0.012964459883268383\n",
            "br 0.012913661404696533\n",
            "\n",
            "state 4\n",
            "<unk> 0.0898155710239038\n",
            ". 0.063231137636617\n",
            "and 0.05117748314288154\n",
            "a 0.047525314526488935\n",
            "to 0.03358549267849003\n",
            "in 0.026387716294359126\n",
            "is 0.026096207148262968\n",
            "of 0.01767035548612207\n",
            "was 0.01609180413102257\n",
            "not 0.015711052484514505\n",
            "\n",
            "state 5\n",
            ". 0.07527909100696237\n",
            ", 0.04508528282744235\n",
            "<unk> 0.04249041117293242\n",
            "of 0.03782986623639959\n",
            "and 0.036167658441353674\n",
            "i 0.03267950172913366\n",
            "is 0.026981443043418138\n",
            "a 0.026114141207581782\n",
            "it 0.023768074506057037\n",
            "in 0.017595230420778273\n",
            "\n",
            "state 6\n",
            "i 0.09440235472092204\n",
            "<unk> 0.054054762566400946\n",
            "the 0.04786765094107878\n",
            "a 0.04378715776602611\n",
            ". 0.0428776660262734\n",
            "and 0.03894268309018315\n",
            "br 0.0338311803739358\n",
            "it 0.02843894065131895\n",
            "my 0.013655680697088645\n",
            "not 0.01330463053509941\n",
            "\n",
            "state 7\n",
            ", 0.05583903556391959\n",
            "and 0.03977887021023833\n",
            "in 0.0302534294640414\n",
            "to 0.02788122734785859\n",
            "for 0.025025994947814566\n",
            "the 0.023147366684598884\n",
            "i 0.019708902167929577\n",
            "my 0.01823133899684085\n",
            "<unk> 0.017766016757181938\n",
            "a 0.017592573373432296\n",
            "\n",
            "state 8\n",
            "i 0.07221485320978387\n",
            ". 0.06541158378714487\n",
            "the 0.046638881601238746\n",
            "<unk> 0.03841302020193987\n",
            "this 0.03549505004468274\n",
            "to 0.027182581106166902\n",
            ", 0.02500946953127829\n",
            "it 0.01942305087024161\n",
            "not 0.013512349164053779\n",
            "and 0.012998651220261457\n",
            "\n",
            "state 9\n",
            ", 0.05231726835856042\n",
            "<unk> 0.05030579715102906\n",
            "the 0.045470643557201866\n",
            "a 0.037444522936492086\n",
            ". 0.03346842648897892\n",
            "and 0.030779588811831297\n",
            "it 0.029599025029012283\n",
            "to 0.02028056109449371\n",
            "i 0.019053400732132516\n",
            "not 0.01720477211231885\n",
            "\n",
            "state 10\n",
            "<unk> 0.13043645678400295\n",
            ". 0.07975871510987152\n",
            ", 0.055488902493307776\n",
            "the 0.03284349774092562\n",
            "br 0.028405102787253708\n",
            "i 0.024389062977084295\n",
            "and 0.021165363776250463\n",
            "is 0.01999822979836433\n",
            "that 0.013717517330436147\n",
            "they 0.01302326725611485\n",
            "\n",
            "state 11\n",
            "<unk> 0.12210230014380977\n",
            ". 0.09929089765291434\n",
            ", 0.05256935271506675\n",
            "i 0.03023415140925048\n",
            "and 0.02457276751977981\n",
            "of 0.024534133339863186\n",
            "that 0.014747553325573934\n",
            "in 0.012996272162790791\n",
            "for 0.012266680323902388\n",
            "to 0.011729512458386214\n",
            "\n",
            "state 12\n",
            "<unk> 0.19131793790929746\n",
            "the 0.048880551436851695\n",
            "this 0.04239459878564892\n",
            "i 0.03944813859386762\n",
            "it 0.03195576651559617\n",
            "of 0.02152590522949218\n",
            "to 0.02054906198253551\n",
            "a 0.01569347188647533\n",
            "is 0.012199726163016303\n",
            "are 0.011439417426358988\n",
            "\n",
            "state 13\n",
            ". 0.11042685294937572\n",
            ", 0.06606814591019708\n",
            "to 0.04168872717974999\n",
            "<unk> 0.03861452887883402\n",
            "br 0.0380035398962323\n",
            "the 0.027805297082229903\n",
            "it 0.02206608971959026\n",
            "i 0.019471758390200704\n",
            "a 0.017789496380286176\n",
            "with 0.015851692258264383\n",
            "\n",
            "state 14\n",
            "<unk> 0.08950648315147894\n",
            ", 0.07833396022143799\n",
            "the 0.041013082723846794\n",
            "and 0.03948550093787296\n",
            "to 0.03889567574196114\n",
            "a 0.027278323844947303\n",
            "of 0.02448901758477\n",
            "this 0.020022812360009124\n",
            "br 0.018331703679673524\n",
            ". 0.013764322131727357\n",
            "\n",
            "state 15\n",
            "<unk> 0.1054070844459089\n",
            ", 0.06882536238413492\n",
            "the 0.0382553438566142\n",
            "to 0.029017578540765802\n",
            "and 0.02901112125893665\n",
            "is 0.02002735856655722\n",
            "but 0.01990496995461105\n",
            "! 0.017091941262126355\n",
            "it 0.014809312317800482\n",
            "not 0.014801014531107949\n",
            "\n",
            "state 16\n",
            "<unk> 0.10759925850060434\n",
            "the 0.07501489057796816\n",
            ". 0.0732859123184837\n",
            "a 0.04366701094606771\n",
            "it 0.03368410026458485\n",
            ", 0.02731001642525638\n",
            "in 0.022409633914739605\n",
            "is 0.021870252644507315\n",
            "and 0.015139800549119224\n",
            "that 0.013579781917550222\n",
            "\n",
            "state 17\n",
            ", 0.06744892573629273\n",
            "of 0.03961678879189001\n",
            "the 0.03579906335484142\n",
            "a 0.032086340368718796\n",
            "i 0.027674927265122037\n",
            "it 0.021093261271356228\n",
            "is 0.020223113762470275\n",
            ". 0.019417837931941025\n",
            "for 0.018011445621546794\n",
            "you 0.01644009472459928\n",
            "\n",
            "state 18\n",
            ". 0.07708782443074592\n",
            "the 0.06508154897721052\n",
            "i 0.04448449805587544\n",
            "a 0.042951074905443136\n",
            "it 0.03374762812257114\n",
            "of 0.030172167592315155\n",
            "is 0.028404063157722995\n",
            "this 0.025250249490764157\n",
            "br 0.016677940470357658\n",
            ", 0.015390791601715203\n",
            "\n",
            "state 19\n",
            ". 0.10419999577701097\n",
            "<unk> 0.09978984415493886\n",
            "a 0.038555935079954806\n",
            ", 0.03719850626374941\n",
            "of 0.03162537688689429\n",
            "it 0.029491451243302297\n",
            "i 0.015756465599649237\n",
            "is 0.014197337641339104\n",
            "in 0.0133645713916965\n",
            "this 0.013116156257430112\n",
            "\n",
            "state 20\n",
            ", 0.05442213424244397\n",
            ". 0.04033529919178106\n",
            "to 0.03702746053386291\n",
            "br 0.023368082986479216\n",
            "are 0.022402721158774452\n",
            "is 0.02217746164690812\n",
            "<unk> 0.021586981492733342\n",
            "in 0.02043616862653968\n",
            "have 0.018252987328058915\n",
            "not 0.014376359254559925\n",
            "\n",
            "state 21\n",
            "<unk> 0.13264741157163953\n",
            ". 0.08888529111251352\n",
            "the 0.05280898509840406\n",
            ", 0.038292847856927215\n",
            "of 0.0334109560501877\n",
            "a 0.03240842915137756\n",
            "for 0.016998864808399506\n",
            "it 0.015417452867261067\n",
            "is 0.014196219617548392\n",
            "and 0.012985752445812895\n",
            "\n",
            "state 22\n",
            "<unk> 0.1957512114038947\n",
            ". 0.08461956506962792\n",
            "the 0.06031560767556544\n",
            "and 0.027685090357351817\n",
            ", 0.0236884625543751\n",
            "of 0.021070600444743817\n",
            "to 0.01317409065030281\n",
            "for 0.012235589390020908\n",
            "it 0.011385859320763914\n",
            "that 0.009380312807952183\n",
            "\n",
            "state 23\n",
            "the 0.06820021557200823\n",
            "<unk> 0.06473884978020801\n",
            "and 0.05329697396399862\n",
            ". 0.04231361108024892\n",
            "to 0.03993295965355895\n",
            "is 0.023602702454686045\n",
            "of 0.016315043590147675\n",
            "a 0.01219085838422458\n",
            ", 0.011793892947370736\n",
            "not 0.010829300856542682\n",
            "\n",
            "state 24\n",
            ". 0.1289069009617367\n",
            "i 0.047699209552439205\n",
            "the 0.04434461053611091\n",
            "<unk> 0.03887599080122308\n",
            "br 0.03577511130105288\n",
            "is 0.025704679199486775\n",
            ", 0.019463103669911202\n",
            "with 0.014761472540175437\n",
            "to 0.014130584130276777\n",
            "this 0.01338943485040459\n",
            "\n",
            "state 25\n",
            "i 0.09497507348099385\n",
            "the 0.054591940136402735\n",
            "<unk> 0.04959744355680217\n",
            "it 0.036311673778699614\n",
            "and 0.03375985412567949\n",
            ". 0.0319277309704274\n",
            "to 0.023229728669904175\n",
            ", 0.02311942768725358\n",
            "a 0.021763530557924383\n",
            "br 0.020392351327953116\n",
            "\n",
            "state 26\n",
            ". 0.08616402705386458\n",
            ", 0.03925266964564496\n",
            "and 0.03317352760611349\n",
            "this 0.030229497452084163\n",
            "is 0.02662299295479036\n",
            "<unk> 0.02554698760739387\n",
            "the 0.024025718963352538\n",
            "a 0.02364871023708196\n",
            "i 0.02196797036798765\n",
            "to 0.019702800266698066\n",
            "\n",
            "state 27\n",
            "<unk> 0.11821259038798303\n",
            "the 0.048325649506538684\n",
            ", 0.03206989856201883\n",
            "of 0.03139725877841939\n",
            "it 0.02880078820185752\n",
            "to 0.02313070090007937\n",
            "is 0.022247831782450378\n",
            "a 0.021880831694300645\n",
            ". 0.01834874106234143\n",
            "i 0.016669505638551088\n",
            "\n",
            "state 28\n",
            ". 0.11568976210262111\n",
            "the 0.06750063614432902\n",
            "<unk> 0.054112966516865875\n",
            "br 0.03300827290856748\n",
            "a 0.031871675228422935\n",
            "to 0.02565209149673838\n",
            "of 0.019712762729358974\n",
            "and 0.012711774875459521\n",
            "that 0.012669522888183935\n",
            "is 0.012204477670145647\n",
            "\n",
            "state 29\n",
            "<unk> 0.1553569073906132\n",
            ", 0.05302397467001936\n",
            "the 0.049617835966270854\n",
            "this 0.02428035879621469\n",
            "and 0.021893356237657213\n",
            "is 0.021778330058086267\n",
            "of 0.015443884132705825\n",
            "to 0.01467297059399048\n",
            "in 0.014594843145687127\n",
            "a 0.014583369305050657\n",
            "\n",
            "state 30\n",
            "<unk> 0.11412534317028884\n",
            ", 0.07712128510221529\n",
            ". 0.061730119168866504\n",
            "the 0.03159445091234894\n",
            "it 0.03067332289115271\n",
            "to 0.01811632604461964\n",
            "in 0.01321450974712446\n",
            "that 0.013141386576419195\n",
            "is 0.013017604905587161\n",
            "for 0.012541789560830567\n",
            "\n",
            "state 31\n",
            ". 0.0903619371563553\n",
            "<unk> 0.07250536701282351\n",
            ", 0.05671738893093779\n",
            "of 0.030592941955571114\n",
            "a 0.02574071774423671\n",
            "the 0.023683960732158137\n",
            "to 0.020321351786856345\n",
            "and 0.012991631661520767\n",
            "for 0.012242902688169858\n",
            "br 0.010265203181707011\n",
            "\n",
            "state 32\n",
            "<unk> 0.08653777051454081\n",
            ". 0.05742591414500421\n",
            "i 0.05337963852062644\n",
            "the 0.0459613529713838\n",
            ", 0.04268543917758257\n",
            "and 0.026699365571669825\n",
            "to 0.026660268255970877\n",
            "is 0.021389710852502793\n",
            "a 0.019799088056100807\n",
            "br 0.015274240387978666\n",
            "\n",
            "state 33\n",
            "i 0.04567635188635884\n",
            ". 0.03808481636206712\n",
            "a 0.03678157487721447\n",
            ", 0.03552152376696268\n",
            "the 0.03239144489623098\n",
            "and 0.03203370026354512\n",
            "to 0.026711670280189965\n",
            "this 0.026447005093795124\n",
            "<unk> 0.024539604142694068\n",
            "in 0.020517933491694968\n",
            "\n",
            "state 34\n",
            "<unk> 0.1097142121766752\n",
            "i 0.05706507859917726\n",
            ", 0.052885782256799144\n",
            ". 0.05200332456594608\n",
            "and 0.03819442020895518\n",
            "br 0.03766636389088792\n",
            "a 0.023847846615785172\n",
            "it 0.019397818763901283\n",
            "of 0.018791548039613023\n",
            "in 0.018305961629649117\n",
            "\n",
            "state 35\n",
            ". 0.07804044468723975\n",
            "<unk> 0.06135110171373102\n",
            "br 0.03680502321040091\n",
            "this 0.021101401294690035\n",
            "to 0.01850753456612111\n",
            "for 0.017809733220752647\n",
            "but 0.017026094437054915\n",
            "i 0.01668810060562197\n",
            "they 0.01579640052052721\n",
            "and 0.015593758070113236\n",
            "\n",
            "state 36\n",
            ". 0.11677525603489004\n",
            "<unk> 0.10221812897959065\n",
            ", 0.045304551871304576\n",
            "and 0.042394706052097254\n",
            "i 0.03895279890953993\n",
            "the 0.03074585669387322\n",
            "it 0.023380628228817035\n",
            "in 0.01816491717519267\n",
            "a 0.013966379164355327\n",
            "of 0.013790356295237335\n",
            "\n",
            "state 37\n",
            ". 0.07553658448203387\n",
            "the 0.054946192687985194\n",
            "a 0.05073589491111157\n",
            "i 0.024706508660935173\n",
            "it 0.02332205223721177\n",
            "br 0.020699223578338297\n",
            "you 0.015881848378360233\n",
            "this 0.01585982524210169\n",
            "was 0.014824182602878551\n",
            "but 0.01339719715572706\n",
            "\n",
            "state 38\n",
            ". 0.11155902541254276\n",
            "a 0.0345447891036453\n",
            "the 0.029377800667401796\n",
            "it 0.02552629635228693\n",
            "for 0.024287994473825576\n",
            "this 0.023556097001408167\n",
            ", 0.02258174625838138\n",
            "and 0.021871313089426376\n",
            "i 0.0203937185263239\n",
            "in 0.01579851999632675\n",
            "\n",
            "state 39\n",
            "the 0.08111417936228824\n",
            ". 0.07219908119386034\n",
            "<unk> 0.04891379714542026\n",
            "i 0.04225628436467083\n",
            "and 0.037588092204266926\n",
            ", 0.025826442484445253\n",
            "it 0.02400747684258407\n",
            "of 0.020214512596756027\n",
            "this 0.019082784163968674\n",
            "a 0.015552932874820017\n",
            "\n",
            "state 40\n",
            "<unk> 0.10692584502115975\n",
            ". 0.09792533799466491\n",
            "i 0.046742362014538755\n",
            "and 0.029869177143390078\n",
            "to 0.025219468436195873\n",
            "br 0.024555782596663904\n",
            "of 0.023596886033838204\n",
            "the 0.02321477207248706\n",
            ", 0.02165523360519356\n",
            "a 0.014532811445546544\n",
            "\n",
            "state 41\n",
            "<unk> 0.12011927295606283\n",
            "the 0.053377947009396486\n",
            "and 0.052998689592164194\n",
            "a 0.04312928949959183\n",
            "i 0.02393037137339588\n",
            "for 0.01733310036156482\n",
            "you 0.014553057184064022\n",
            "to 0.014208776137815128\n",
            "is 0.013647769343789445\n",
            "with 0.012958873226863147\n",
            "\n",
            "state 42\n",
            "a 0.04714371879655559\n",
            "it 0.03378223893305346\n",
            "i 0.03218591625123606\n",
            "to 0.02562750574088384\n",
            ", 0.025398538260956293\n",
            "the 0.022079419524960314\n",
            "my 0.01571678598985893\n",
            "<unk> 0.015351187826544923\n",
            "! 0.01505785171341468\n",
            "and 0.014826547260633505\n",
            "\n",
            "state 43\n",
            "<unk> 0.1179951415532188\n",
            "i 0.04241191213077853\n",
            "the 0.030058611959688\n",
            ", 0.028674109672256393\n",
            "a 0.025860513415047692\n",
            "and 0.01822559151423301\n",
            "these 0.01756518141928197\n",
            "for 0.015367429903409661\n",
            "my 0.013714559820496745\n",
            "of 0.013307340900385368\n",
            "\n",
            "state 44\n",
            ". 0.0915840890287296\n",
            ", 0.05802258505148096\n",
            "the 0.04666994876456948\n",
            "and 0.04111529058217679\n",
            "i 0.04106993006149287\n",
            "to 0.02054635330528549\n",
            "is 0.020400740905947696\n",
            "but 0.01617251226876323\n",
            "<unk> 0.014107529534377987\n",
            "in 0.013664647982415581\n",
            "\n",
            "state 45\n",
            "<unk> 0.1352728334665721\n",
            ". 0.07179126390103999\n",
            "i 0.07163962264554104\n",
            "it 0.02783102708218768\n",
            "a 0.023120022215141915\n",
            "br 0.022945024768671395\n",
            "to 0.02036097190938408\n",
            "the 0.015523118182952086\n",
            "not 0.012812339355579737\n",
            "was 0.012519214591613355\n",
            "\n",
            "state 46\n",
            "<unk> 0.14198024384734598\n",
            ". 0.07782283415126974\n",
            "to 0.03426033008706623\n",
            "and 0.031027392263882937\n",
            "i 0.027949617517757472\n",
            "is 0.024981175085654558\n",
            "a 0.024791979762642684\n",
            "the 0.020852357657639792\n",
            "of 0.01925252076115371\n",
            "br 0.01686552844619547\n",
            "\n",
            "state 47\n",
            ". 0.08651098661378936\n",
            "i 0.04582816294478209\n",
            "<unk> 0.04466248554124663\n",
            "this 0.0415793638070705\n",
            "a 0.03205124030022724\n",
            "the 0.026357140526513438\n",
            "to 0.023426323335232792\n",
            "is 0.02335558952822949\n",
            "of 0.021360017281273617\n",
            "for 0.019162439954678593\n",
            "\n",
            "state 48\n",
            ". 0.09970877260489569\n",
            "i 0.05482159205915553\n",
            "<unk> 0.051453728534145454\n",
            "the 0.024692872578040936\n",
            "br 0.015583309113155357\n",
            "of 0.015212970392467402\n",
            ", 0.01424257132609748\n",
            "a 0.012574438494118007\n",
            "have 0.012077333424432898\n",
            "my 0.011319860678876296\n",
            "\n",
            "state 49\n",
            "<unk> 0.16664920112788995\n",
            "the 0.04982552058309522\n",
            "and 0.030792501436700766\n",
            "a 0.03032793265623367\n",
            ", 0.020406390774689274\n",
            "it 0.01835401985868985\n",
            "this 0.0182912307840841\n",
            "i 0.017132530466109075\n",
            ". 0.01704636768370969\n",
            "br 0.014344381608555175\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCqMRXBrFn0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "7af66629-decf-4897-aa03-639cd862f30f"
      },
      "source": [
        "for i in range(10):\n",
        "    print(tokenizer50.de_tokenize([hmm50.generate(10)]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['allergic a it br over you purchase only about for']\n",
            "['it what one in have chips if <unk> in is']\n",
            "[\"the tastes flowers during when but to and doesn't more\"]\n",
            "['just . this still if i are eaten the i']\n",
            "['with , . br calories high over me when toddler']\n",
            "['crunchy powder since school br again to their of i']\n",
            "['perfect pack . and 10 <unk> , brands high it']\n",
            "['come eat amazon other am have . i much i']\n",
            "['. product i contained <unk> ! looking but coffee away']\n",
            "['packaging is it caramel actually br br with these how']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZyjyECmOFSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c0bc0f2-e272-4751-d357-f093979cc556"
      },
      "source": [
        "train_sizes = [10, 20, 30, 50, 75, 100, 200, 300, 500, 750, 1000, 1500, 2000, 2500, 3000]\n",
        "\n",
        "hmm_50_results = []\n",
        "\n",
        "def hmm50_featurizer(review):\n",
        "    review = review[0]\n",
        "    _, _, gamma = hmm50.forward_backward(review)\n",
        "    return gamma.sum(axis=0)\n",
        "\n",
        "for n in train_sizes:\n",
        "    hmm_50_results.append(training_experiment(\"hmm\", hmm50_featurizer, n_train=n))"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hmm features, 10 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.468\n",
            "\n",
            "hmm features, 20 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.45\n",
            "\n",
            "hmm features, 30 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.444\n",
            "\n",
            "hmm features, 50 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.422\n",
            "\n",
            "hmm features, 75 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.414\n",
            "\n",
            "hmm features, 100 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.508\n",
            "\n",
            "hmm features, 200 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.574\n",
            "\n",
            "hmm features, 300 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.642\n",
            "\n",
            "hmm features, 500 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.602\n",
            "\n",
            "hmm features, 750 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.606\n",
            "\n",
            "hmm features, 1000 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.612\n",
            "\n",
            "hmm features, 1500 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.614\n",
            "\n",
            "hmm features, 2000 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.618\n",
            "\n",
            "hmm features, 2500 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.616\n",
            "\n",
            "hmm features, 3000 examples\n",
            "test accuracy 0.616\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD3zXdRCOHRV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "923f9c96-e35a-45a5-f8bd-b26ab34d3212"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(train_sizes, hmm_50_results, label=\"hmm\")\n",
        "# plt.legend(loc = \"lower right\")\n",
        "# plt.title('Test Accuracy of HMM-based Representations with 50 Hidden States on Different Train Data Sizes')\n",
        "plt.xlabel('Number of Train Examples')\n",
        "plt.ylabel('Test Accuracy')\n",
        "\n",
        "# plt.savefig('hmm_ntrain.png')\n",
        "# files.download('hmm_ntrain.png')\n",
        "plt.show()"
      ],
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAFzCAYAAAAkFp78AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAAhIiIlKUCIiEhJ/x9r+6ezX+PIkQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsXr1CkCCzFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "92fb769f-6022-4797-e50c-833503ac7ed9"
      },
      "source": [
        "tokenizer = lab_util.Tokenizer()\n",
        "tokenizer.fit(train_reviews)\n",
        "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
        "print(tokenizer.vocab_size)\n",
        "\n",
        "hmm100 = HMM(num_states=100, num_words=tokenizer.vocab_size)\n",
        "hmm100.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2006\n",
            "log-likelihood -2073205.3254522625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-329-63586b40b450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhmm100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhmm100\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_unsupervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reviews_tk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-315-d8b6308e364f>\u001b[0m in \u001b[0;36mlearn_unsupervised\u001b[0;34m(self, corpus, num_iters, verbose)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogaddexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_si\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_gamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_si\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogaddexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_sij\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_xi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_sij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogaddexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_sj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_sj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/special/_logsumexp.py\u001b[0m in \u001b[0;36mlogsumexp\u001b[0;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# suppress warnings about log of zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}